# Multi-Modal Retrieval Augmented Generation
A proof-of-concept Multimodal RAG (Retrieval-Augmented Generation) system using ollama, LangChain, and Streamlit to chat with PDFs, images and tables.

## Installation instructions
1. Install python 3.9+ and create a virtual environment.
2. Install Ollama on your local machine from the [official website](https://ollama.com/)
3. Ollama pull your desired model using the ollama supported models.
4. Install dependencies using pip (recommended to use uv package manager) or similar package managers.

## Running the Code
Run the Streamlit app:
streamlit run multi_modal_rag.py

## Built with
ollama
LangChain
Streamlit

## Contact
This project was created by Dr. Nidhi Gowdra at the Centre for eResearch, University of Auckland.
For questions or suggestions, Contact: https://www.eresearch.auckland.ac.nz/getting-in-touch/

## Credits
Thanks to NarimanN2:
https://github.com/NarimanN2/ollama-playground/tree/main/multi-modal-rag

